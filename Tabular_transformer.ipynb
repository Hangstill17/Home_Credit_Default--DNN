{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb7e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torchmetrics.classification import BinaryAUROC \n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler,QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_final=pd.read_csv(\"D:/Desktop/æœºå™¨å­¦ä¹ çš„æ•°æ®é›†/Home Credit Default Risk/train_final_without_ohe.csv\")\n",
    "test_final=pd.read_csv(\"D:/Desktop/æœºå™¨å­¦ä¹ çš„æ•°æ®é›†/Home Credit Default Risk/test_final_without_ohe.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48a1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.columns = [re.sub(r\"[^a-zA-Z0-9_\\-]\",\"_\",str(col)) for col in train_final.columns]\n",
    "test_final.columns = [re.sub(r\"[^a-zA-Z0-9_\\-]\",\"_\",str(col)) for col in test_final.columns]\n",
    "train_final.drop(\"Unnamed__0\",axis=1,inplace=True)\n",
    "test_final.drop(\"Unnamed__0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94dcd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE</th>\n",
       "      <th>...</th>\n",
       "      <th>AMT_CREDIT_SUM_DEBT_SUM_mean</th>\n",
       "      <th>AMT_CREDIT_SUM_DEBT_SUM_max</th>\n",
       "      <th>AMT_CREDIT_SUM_DEBT_SUM_sum</th>\n",
       "      <th>CREDIT_ACTIVE_count</th>\n",
       "      <th>CREDIT_ACTIVE_nunique</th>\n",
       "      <th>CREDIT_CURRENCY_count</th>\n",
       "      <th>CREDIT_CURRENCY_nunique</th>\n",
       "      <th>CREDIT_TYPE_count</th>\n",
       "      <th>CREDIT_TYPE_nunique</th>\n",
       "      <th>Bureau_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M and  Others (new)</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>...</td>\n",
       "      <td>34721.195625</td>\n",
       "      <td>245781.0</td>\n",
       "      <td>277769.565</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>Family</td>\n",
       "      <td>...</td>\n",
       "      <td>202500.000000</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>810000.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M and  Others (new)</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M and  Others (new)</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307506</th>\n",
       "      <td>456251</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M and  Others (new)</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>254700.0</td>\n",
       "      <td>27558.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307507</th>\n",
       "      <td>456252</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>269550.0</td>\n",
       "      <td>12001.5</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307508</th>\n",
       "      <td>456253</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>153000.0</td>\n",
       "      <td>677664.0</td>\n",
       "      <td>29979.0</td>\n",
       "      <td>585000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>...</td>\n",
       "      <td>448958.250000</td>\n",
       "      <td>1624797.0</td>\n",
       "      <td>1795833.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307509</th>\n",
       "      <td>456254</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>171000.0</td>\n",
       "      <td>370107.0</td>\n",
       "      <td>20205.0</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307510</th>\n",
       "      <td>456255</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>49117.5</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>Unaccompanied</td>\n",
       "      <td>...</td>\n",
       "      <td>139537.546364</td>\n",
       "      <td>595102.5</td>\n",
       "      <td>1534913.010</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307511 rows Ã— 495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_CURR NAME_CONTRACT_TYPE           CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0           100002         Cash loans  M and  Others (new)             N   \n",
       "1           100003         Cash loans                     F            N   \n",
       "2           100004    Revolving loans  M and  Others (new)             Y   \n",
       "3           100006         Cash loans                     F            N   \n",
       "4           100007         Cash loans  M and  Others (new)             N   \n",
       "...            ...                ...                   ...          ...   \n",
       "307506      456251         Cash loans  M and  Others (new)             N   \n",
       "307507      456252         Cash loans                     F            N   \n",
       "307508      456253         Cash loans                     F            N   \n",
       "307509      456254         Cash loans                     F            N   \n",
       "307510      456255         Cash loans                     F            N   \n",
       "\n",
       "       FLAG_OWN_REALTY  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0                    Y          202500.0    406597.5      24700.5   \n",
       "1                    N          270000.0   1293502.5      35698.5   \n",
       "2                    Y           67500.0    135000.0       6750.0   \n",
       "3                    Y          135000.0    312682.5      29686.5   \n",
       "4                    Y          121500.0    513000.0      21865.5   \n",
       "...                ...               ...         ...          ...   \n",
       "307506               N          157500.0    254700.0      27558.0   \n",
       "307507               Y           72000.0    269550.0      12001.5   \n",
       "307508               Y          153000.0    677664.0      29979.0   \n",
       "307509               Y          171000.0    370107.0      20205.0   \n",
       "307510               N          157500.0    675000.0      49117.5   \n",
       "\n",
       "        AMT_GOODS_PRICE NAME_TYPE_SUITE  ... AMT_CREDIT_SUM_DEBT_SUM_mean  \\\n",
       "0              351000.0   Unaccompanied  ...                 34721.195625   \n",
       "1             1129500.0          Family  ...                202500.000000   \n",
       "2              135000.0   Unaccompanied  ...                     0.000000   \n",
       "3              297000.0   Unaccompanied  ...                     0.000000   \n",
       "4              513000.0   Unaccompanied  ...                     0.000000   \n",
       "...                 ...             ...  ...                          ...   \n",
       "307506         225000.0   Unaccompanied  ...                     0.000000   \n",
       "307507         225000.0   Unaccompanied  ...                     0.000000   \n",
       "307508         585000.0   Unaccompanied  ...                448958.250000   \n",
       "307509         319500.0   Unaccompanied  ...                     0.000000   \n",
       "307510         675000.0   Unaccompanied  ...                139537.546364   \n",
       "\n",
       "       AMT_CREDIT_SUM_DEBT_SUM_max AMT_CREDIT_SUM_DEBT_SUM_sum  \\\n",
       "0                         245781.0                  277769.565   \n",
       "1                         810000.0                  810000.000   \n",
       "2                              0.0                       0.000   \n",
       "3                              0.0                       0.000   \n",
       "4                              0.0                       0.000   \n",
       "...                            ...                         ...   \n",
       "307506                         0.0                       0.000   \n",
       "307507                         0.0                       0.000   \n",
       "307508                   1624797.0                 1795833.000   \n",
       "307509                         0.0                       0.000   \n",
       "307510                    595102.5                 1534913.010   \n",
       "\n",
       "       CREDIT_ACTIVE_count  CREDIT_ACTIVE_nunique  CREDIT_CURRENCY_count  \\\n",
       "0                      8.0                    2.0                    8.0   \n",
       "1                      4.0                    2.0                    4.0   \n",
       "2                      2.0                    1.0                    2.0   \n",
       "3                      0.0                    0.0                    0.0   \n",
       "4                      1.0                    1.0                    1.0   \n",
       "...                    ...                    ...                    ...   \n",
       "307506                 0.0                    0.0                    0.0   \n",
       "307507                 0.0                    0.0                    0.0   \n",
       "307508                 4.0                    2.0                    4.0   \n",
       "307509                 1.0                    1.0                    1.0   \n",
       "307510                11.0                    2.0                   11.0   \n",
       "\n",
       "        CREDIT_CURRENCY_nunique  CREDIT_TYPE_count  CREDIT_TYPE_nunique  \\\n",
       "0                           1.0                8.0                  2.0   \n",
       "1                           1.0                4.0                  2.0   \n",
       "2                           1.0                2.0                  1.0   \n",
       "3                           0.0                0.0                  0.0   \n",
       "4                           1.0                1.0                  1.0   \n",
       "...                         ...                ...                  ...   \n",
       "307506                      0.0                0.0                  0.0   \n",
       "307507                      0.0                0.0                  0.0   \n",
       "307508                      1.0                4.0                  2.0   \n",
       "307509                      1.0                1.0                  1.0   \n",
       "307510                      1.0               11.0                  2.0   \n",
       "\n",
       "        Bureau_flag  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 0  \n",
       "4                 1  \n",
       "...             ...  \n",
       "307506            0  \n",
       "307507            0  \n",
       "307508            1  \n",
       "307509            1  \n",
       "307510            1  \n",
       "\n",
       "[307511 rows x 495 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6820028",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_final.fillna(0,inplace=True)\n",
    "df = train_final.copy()\n",
    "\n",
    "# åˆ†ç¦»æ•°å€¼åˆ—å’Œç±»åˆ«åˆ—..ç®€å•çš„é€šè¿‡è¿™é‡Œçš„åˆ¤æ–­ç±»å‹å¾—åˆ°çš„åˆ†ç±»ç‰¹å¾è¿˜æ˜¯å¤ªå°‘ï¼Œæ‰€ä»¥è¿™æ—¶å€™è¿˜æ˜¯éœ€è¦è¿›è¡Œç­›é€‰\n",
    "categorical_cols = df.select_dtypes(include=['object',\"O\"]).columns.tolist()\n",
    "numerical_cols = [c  for c in df.columns if c not in categorical_cols]\n",
    "\n",
    "# ç§»é™¤ ID åˆ—ï¼ˆå¦‚ SK_ID_CURRï¼‰\n",
    "if 'SK_ID_CURR' in numerical_cols:\n",
    "    numerical_cols.remove('SK_ID_CURR')\n",
    "nunique_counts = train_final.nunique()\n",
    "# ç­›é€‰å‡ºå”¯ä¸€å€¼æ•°é‡ < 20 çš„åˆ—\n",
    "low_cardinality = nunique_counts[nunique_counts < 60][1:]\n",
    "# è½¬ä¸º DataFrame å¹¶å‘½ååˆ—\n",
    "count_df = low_cardinality.to_frame(name='NUMBER_COLUMNS')\n",
    "length=[]\n",
    "for col  in count_df.index:\n",
    "    if col not in categorical_cols:\n",
    "        length.append(col)\n",
    "len(length)\n",
    "\n",
    "categorical_cols=[c for c in length if c!=\"TARGET\"]\n",
    "numerical_cols=[c for c in numerical_cols if c not in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f6b118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(categorical_cols))\n",
    "len(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d19fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=train_final.TARGET\n",
    "ids = test_final.SK_ID_CURR\n",
    "#print(test_final.head())\n",
    "train=df.drop([\"SK_ID_CURR\",\"TARGET\"],axis=1)\n",
    "test_final.drop([\"SK_ID_CURR\"],axis=1,inplace=True)\n",
    "cols=[\"TARGET\",'SK_ID_PREV','SK_ID_BUREAU','SK_ID_CURR']\n",
    "numerical_cols=[col for col in numerical_cols if col not in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1adb5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(train,target,test_size = 0.10, random_state = 1008, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a4e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¥å£®æ€§çš„æ ‡ç­¾ç¼–ç æ ¼å¼\n",
    "class RobustLabelEncoder:\n",
    "    def __init__(self,min_obs=1):\n",
    "        self.min_obs=min_obs\n",
    "        self.mapper={}\n",
    "        self.cate_dims = 0\n",
    "    def fit(self,series):\n",
    "        value_counts=series.value_counts()\n",
    "        valid_cols=value_counts[value_counts>self.min_obs].index.tolist()\n",
    "        # æ„å»ºæ˜ å°„è¡¨,ä»1å¼€å§‹æ˜ å°„\n",
    "        self.mapper={val:i+1 for i,val in enumerate(valid_cols)}\n",
    "        self.cate_dims=len(self.mapper)+1       # è®°å½•ç±»å‹æ ‡ç­¾çš„é•¿åº¦\n",
    "        return self\n",
    "    def transform(self,series):\n",
    "        return series.map(self.mapper).fillna(0).astype(int).values   # å¯¹äºåœ¨fitä¸­æ²¡æœ‰å‡ºç°çš„ç±»å‹ï¼Œå¡«å……ä¸º0ï¼Œè¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆä¸Šé¢çš„fitæ˜¯ä»i+1å¼€å§‹çš„\n",
    "    def fit_transform(self,series):\n",
    "        self.fit(series)\n",
    "        return self.transform(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56669b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆ†ç±»ç‰¹å¾ç»´åº¦: [3, 3, 3, 3, 15, 4, 25, 3, 3, 3, 30, 8, 29, 8, 3, 3, 3, 5, 9, 10, 21, 10, 21, 3, 6, 5, 26, 21, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 48, 30, 13, 33, 4, 4, 4, 4, 5, 18, 3, 5, 32, 3, 5, 9, 9, 3, 8, 3, 44, 4, 4, 4, 3]\n",
      "åˆ†ç±»ç‰¹å¾æ•°é‡: 99\n"
     ]
    }
   ],
   "source": [
    "# è¿™æ˜¯å¯¹æ•°å€¼æ€§ç‰¹å¾è¿›è¡Œçš„å¤„ç†ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸ç®¡æ˜¯ä»€ä¹ˆå½¢å¼çš„åˆ†å¸ƒï¼Œå°¤å…¶æ˜¯æ¶‰åŠåˆ°é’±çš„å³ååˆ†å¸ƒï¼Œéƒ½é€‚åˆæŠŠè¿™é‡Œå˜æˆæ­£æ€åˆ†å¸ƒã€‚\n",
    "scaler=QuantileTransformer(n_quantiles=2000,                 # é‡‡æ ·çš„ä»½æ•°ï¼Œè¶Šå¤§è¶Šç²¾ç¡®ï¼Œä½†å®¹æ˜“è¿‡æ‹Ÿåˆã€‚å¯¹äºå¤§è¿™ç±»æ•°æ®ï¼Œè®¾ä¸º 2000 æˆ– len(train) éƒ½å¯ä»¥\n",
    "                           output_distribution='normal',    # å˜æˆé«˜æ–¯åˆ†å¸ƒçš„å…³é”®\n",
    "                           random_state=108)\n",
    "x_train_numerical=x_train[numerical_cols].copy()\n",
    "x_test_numerical=x_test[numerical_cols].copy()\n",
    "train_new_cols={}\n",
    "test_new_cols={}\n",
    "for col in numerical_cols:   # é˜²æ­¢æŸäº›æ•°å€¼æ€§å­—æ®µå‡ºç°æ— ç©·å¤§çš„ï¼Œå°±æ²¡æœ‰åŠæ³•è¿›è¡Œæ ‡ç­¾ç¼–ç \n",
    "    x_train_numerical[col] = x_train_numerical[col].replace([np.inf, -np.inf], np.nan)\n",
    "    x_test_numerical[col] = x_test_numerical[col].replace([np.inf, -np.inf], np.nan)\n",
    "    if x_train_numerical[col].isnull().any():    #å¦‚æœä¸€åˆ—ä¹‹ä¸­æœ‰ç¼ºå¤±å€¼å­˜åœ¨\n",
    "         # åˆ›å»ºä¸€ä¸ªæ–°åˆ—æ ‡æ³¨æ˜¯å¦ç¼ºå¤±.å…ˆä½¿ç”¨ä¸€ä¸ªå­—å…¸å­˜å‚¨å¥½æ–°çš„åˆ—ï¼Œè¿™æ ·ä¸ç”¨åå¤çš„è°ƒç”¨æœ¬æ¥å¾ˆå¤§çš„çš„x_train_numerical\n",
    "         train_new_cols[f\"{col}_is_NaN\"]=x_train_numerical[col].isnull().astype(int)  # astype()å¯ä»¥æŠŠboolå€¼å˜æˆæ•°å­—1ï¼Œ0\n",
    "         test_new_cols[f\"{col}_is_NaN\"]=x_test_numerical[col].isnull().astype(int) \n",
    "         # æ­¤æ—¶å¯¹ç¼ºå¤±å€¼è¿›è¡Œå¡«å……ã€‚ç”¨ä¸­ä½æ•°å¡«å……ä¼šå¿½ç•¥åˆ°ä¸å¡«æ”¶å…¥çš„äººçš„è¿çº¦é£é™©ï¼Œæ‰€ä»¥åœ¨å¡«å……å‰è¿›è¡Œäººå‘˜çš„æ ‡æ³¨\n",
    "         med = x_train_numerical[col].median()\n",
    "         if pd.isna(med):\n",
    "             med=0\n",
    "         x_train_numerical[col] = x_train_numerical[col].fillna(med)\n",
    "         x_test_numerical[col] = x_test_numerical[col].fillna(med)\n",
    "x_train_numerical =pd.concat([x_train_numerical,pd.DataFrame(train_new_cols)],axis=1)\n",
    "numerical_cols=x_train_numerical.columns.to_list()\n",
    "x_test_numerical  =pd.concat([x_test_numerical,pd.DataFrame(test_new_cols)],axis=1)\n",
    "x_train_numerical = scaler.fit_transform(x_train_numerical)  # è¿”å›çš„æ˜¯numpyçš„æ ¼å¼\n",
    "x_test_numerical  = scaler.transform(x_test_numerical)\n",
    "\n",
    "\n",
    "# å¯¹éæ•°å€¼æ€§ç‰¹å¾è¿›è¡Œå¤„ç†\n",
    "cate_dims_ls=[]\n",
    "valid_categorical_cols = [] \n",
    "x_train_cate_encoded = x_train[categorical_cols].copy()\n",
    "x_test_cate_encoded = x_test[categorical_cols].copy()\n",
    "for col in categorical_cols:\n",
    "     x_train_cate_encoded[col]=x_train_cate_encoded[col].fillna(\"MISSING_VAL\").astype(str)\n",
    "     x_test_cate_encoded[col]=x_test_cate_encoded[col].fillna(\"MISSING_VAL\").astype(str)\n",
    "     encoder=RobustLabelEncoder(min_obs=1)\n",
    "     encoder.fit(x_train_cate_encoded[col])\n",
    "    #åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Š TRANSFORM\n",
    "    # å…³é”®ç‚¹ï¼šå¦‚æœ x_test é‡Œå‡ºç°äº† x_train é‡Œæ²¡æœ‰çš„ç±»åˆ«ï¼Œä¼šè‡ªåŠ¨å˜ä¸º 0\n",
    "     x_train_cate_encoded[col] = encoder.transform(x_train_cate_encoded[col])\n",
    "     x_test_cate_encoded[col] = encoder.transform(x_test_cate_encoded[col])\n",
    "     max_index_train =x_train_cate_encoded[col].max()\n",
    "     max_index_test=x_test_cate_encoded[col].max()\n",
    "     num_embeddings = max(int(max_index_train),int(max_index_test)) + 1\n",
    "     #print(f\"{col}_dim_{num_embeddings}\")\n",
    "     if num_embeddings>100 or num_embeddings==1:\n",
    "          continue\n",
    "     cate_dims_ls.append(num_embeddings)\n",
    "     valid_categorical_cols.append(col)\n",
    "print(\"åˆ†ç±»ç‰¹å¾ç»´åº¦:\", cate_dims_ls)\n",
    "print(\"åˆ†ç±»ç‰¹å¾æ•°é‡:\", len(cate_dims_ls))\n",
    "\n",
    "# è®¾ç½®æ¯ä¸ªç±»åˆ«ç‰¹å¾çš„åµŒå…¥ç»´åº¦ï¼ˆè¿™æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼‰\n",
    "# ç»éªŒæ³•åˆ™: emb_dim = min(50, (cardinality + 2) // 2)\n",
    "emb_dims = [(x, min(50, (x + 2) // 2)) for x in cate_dims_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4407f391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.58460315,  1.31029799,  0.68195677, ..., -5.19933758,\n",
       "        -5.19933758, -5.19933758],\n",
       "       [-0.18602365,  1.13201337,  0.51750683, ..., -5.19933758,\n",
       "        -5.19933758, -5.19933758],\n",
       "       [ 0.67330956, -0.76005182, -0.27130367, ..., -5.19933758,\n",
       "        -5.19933758, -5.19933758],\n",
       "       ...,\n",
       "       [ 0.14217286,  1.33129627,  0.7112196 , ..., -5.19933758,\n",
       "        -5.19933758, -5.19933758],\n",
       "       [-0.18602365,  0.48345357,  0.27897087, ..., -5.19933758,\n",
       "        -5.19933758, -5.19933758],\n",
       "       [-1.29130396, -1.87327431, -1.07491097, ..., -5.19933758,\n",
       "        -5.19933758, -5.19933758]], shape=(276759, 369))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6622a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwapNoise(nn.Module):\n",
    "    def __init__(self, probability=0.15):  # è¿™é‡Œçš„0.15æ˜¯è¯´Batch\n",
    "        super().__init__()\n",
    "        self.probability = probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        # ç”Ÿæˆæ©ç ï¼š1ä»£è¡¨ä¿æŒï¼Œ0ä»£è¡¨äº¤æ¢\n",
    "        # torch.full_like(x, 1 - self.probability)ç”Ÿæˆäº†å’Œxå½¢çŠ¶å®Œå…¨ä¸€æ ·çš„å…¨ä¸º0.85çš„å¼ é‡\n",
    "        #mask = torch.bernoulli(torch.full_like(x, 1 - self.probability))\n",
    "        mask =torch.rand_like(x.float()) > self.probability\n",
    "        \n",
    "        # åœ¨ Batch å†…éƒ¨éšæœºæ‰“ä¹±æ ·æœ¬é¡ºåº\n",
    "        batch_size = x.size(0)\n",
    "        # torch.randperm(n)`ï¼šè¿”å› `[0, 1, ..., n-1]` çš„ä¸€ä¸ªéšæœºæ’åˆ—\n",
    "        idx = torch.randperm(batch_size, device=x.device)\n",
    "        shuffled_x = x[idx]\n",
    "        \n",
    "        # æ··åˆï¼šåŸæ•°æ® * æ©ç  + æ‰“ä¹±æ•°æ® * (1-æ©ç )\n",
    "        #return x * mask + shuffled_x * (1 - mask)\n",
    "        return torch.where(mask,x,shuffled_x)\n",
    "\n",
    "class Tabular_Transformer(nn.Module):\n",
    "    def __init__(self, num_numerical, cate_dims_ls, dropout=0.10,num_blocks=3,embed_dim=6):\n",
    "        super().__init__()\n",
    "        self.swap_noise = SwapNoise(probability=0.15)\n",
    "        self.embed_dim=embed_dim\n",
    "        self.cate_embedding_layers = nn.ModuleList([\n",
    "            nn.Embedding(num, embed_dim) for num in cate_dims_ls\n",
    "        ])\n",
    "\n",
    "        #ä¸ºæ¯ä¸ªæ•°å€¼ç‰¹å¾å‡†å¤‡ä¸€ä¸ªä¸“ç”¨çš„ Linear å±‚\n",
    "        self.num_projections = nn.ModuleList([\n",
    "            nn.Linear(1, embed_dim) for _ in range(num_numerical)\n",
    "        ])\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=8,            # 8å¤´æ³¨æ„åŠ›\n",
    "            dim_feedforward=embed_dim * 2,   # å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ä¸­é—´å±‚çš„ç»´åº¦\n",
    "            dropout=dropout, \n",
    "            activation='gelu',  # å‰é¦ˆç½‘ç»œä¸­çš„æ¿€æ´»å‡½æ•° \n",
    "            batch_first=True    # è¾“å…¥å¼ é‡çš„æ ¼å¼ï¼šTrue: (batch_size, seq_len, d_model).ã€‚ã€‚æ˜¯ä¸ºäº†å’ŒResNretå¥½é“¾æ¥\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_blocks)\n",
    "        self.cls_token=nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "\n",
    "        # --- Decoder (åªç”¨äºé¢„è®­ç»ƒ) ---\n",
    "        # ç›®æ ‡æ˜¯è¿˜åŸå› total_dim ç»´åº¦çš„åŸå§‹è¾“å…¥\n",
    "        self.num_decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim,1) \n",
    "        )\n",
    "        self.cate_decoders=nn.ModuleList()\n",
    "        for num_cate in cate_dims_ls:\n",
    "            cate_decoder=nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim,num_cate) )\n",
    "            self.cate_decoders.append(cate_decoder)\n",
    "\n",
    "        # ç”¨äºæœ€ç»ˆåˆ†ç±»çš„è¿‡ç¨‹\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(embed_dim, 1)\n",
    "        )\n",
    "\n",
    "    def embed_total_dims(self,x_num,x_cate):\n",
    "        x_num_embed=[]\n",
    "        for i,proj in enumerate(self.num_projections):\n",
    "            col=x_num[:,i].unsqueeze(1)  # (Batch_size,1)\n",
    "            proj\n",
    "            x_num_embed.append(proj(col)) # (Batch_size,num_features)\n",
    "        x_num_embed=torch.stack(x_num_embed,dim=1)  # torch.stack()ä¼šæ²¿æ–°ç»´åº¦ dim=1 å †å è¿™ N ä¸ª (B, d) å¼ é‡ã€‚  ï¼ˆBï¼Œnum_features,embedï¼‰\n",
    "        #print(x_num_embed.shape)\n",
    "\n",
    "        x_cate_embed=[]\n",
    "        for i,proj in enumerate(self.cate_embedding_layers):\n",
    "            col=x_cate[:,i].unsqueeze(1)   # (Batch_size,1)\n",
    "            proj_col=proj(col).squeeze(1)  # proj(col)æ˜¯ï¼ˆbatch_SIZE,1ï¼Œembed_dimï¼‰,æ‰€ä»¥éœ€è¦æŠŠå®ƒå…ˆå‹æ‰\n",
    "            #print(proj(col).shape)\n",
    "            x_cate_embed.append(proj_col) # (Batch_size,num_features)\n",
    "        x_cate_embed=torch.stack(x_cate_embed,dim=1)  # torch.stack()ä¼šæ²¿æ–°ç»´åº¦ dim=1 å †å è¿™ N ä¸ª (B, d) å¼ é‡ã€‚  ï¼ˆBï¼Œnum_features,embedï¼‰\n",
    "        # print(x_cate.shape)        # ([1024,102])\n",
    "        # print(x_cate_embed.shape)  # ([1024, 102, 1, 32])\n",
    "\n",
    "        x=torch.cat([x_num_embed,x_cate_embed],dim=1)\n",
    "        return x\n",
    "    def forward_pretrain(self,x_cate,x_num):\n",
    "        x_cate_noisy = self.swap_noise(x_cate)\n",
    "        x_num_noisy = self.swap_noise(x_num)\n",
    "        x=self.embed_total_dims(x_num=x_num_noisy,x_cate=x_cate_noisy)\n",
    "\n",
    "        cls_token=self.cls_token.expand(x.shape[0],-1,-1)\n",
    "        x=torch.cat([cls_token,x],dim=1)\n",
    "        x = self.transformer(x)\n",
    "        x_features=x[:,1:,:]     # æˆ‘ä»¬ä¸éœ€è¦ CLS Token çš„è¾“å‡ºæ¥åšé‡å»ºï¼Œåªå–åé¢çš„ç‰¹å¾éƒ¨åˆ†\n",
    "\n",
    "        # æ‹†åˆ†ä¸ºæ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾\n",
    "        n_num=x_num_noisy.shape[1]\n",
    "        x_num_out = x_features[:, :n_num, :] # (Batch, N_num, embed_dim)\n",
    "        x_cat_out = x_features[:, n_num:, :] # (Batch, N_cat, embed_dims)\n",
    "\n",
    "       \n",
    "        pred_num_list = self.num_decoder(x_num_out).squeeze(-1) # (Batch, N_num)\n",
    "        #print(pred_num_list)\n",
    "        \n",
    "        # B. é¢„æµ‹ç±»åˆ«ç‰¹å¾ (åˆ†ç±»)\n",
    "        pred_cat_list = []\n",
    "        # for i, decoder in enumerate(self.cate_decoders):\n",
    "        #     # è¾“å…¥ç¬¬ i ä¸ªç±»åˆ«ç‰¹å¾çš„ embeddingï¼Œé¢„æµ‹ç±»åˆ« logits\n",
    "        #     pred_logits = decoder(x_cat_out[:, i, :]) # (Batch, Num_Classes)\n",
    "        #     pred_cat_list.append(pred_logits)\n",
    "        return pred_num_list\n",
    "        \n",
    "\n",
    "    def forward_classify(self,x_cate,x_num):\n",
    "        x=self.embed_total_dims(x_num,x_cate)\n",
    "        cls_token=self.cls_token.expand(x.shape[0],-1,-1)\n",
    "        x=torch.cat([cls_token,x],dim=1)\n",
    "        x = self.transformer(x)\n",
    "        cls_output=x[:,0,:]     # è¿™ä¸ªå…¶å®å°±æ˜¯cls_tokenæ±‡æ€»çš„ä¿¡æ¯é‡\n",
    "        logitis=self.classifier_head(cls_output)\n",
    "\n",
    "        return logitis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faf9c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_numerical_df=pd.DataFrame(scaler.fit_transform(x_train_numerical),columns=numerical_cols)\n",
    "x_test_numerical_df =pd.DataFrame(scaler.transform(x_test_numerical),columns=numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "895c01e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213080    0\n",
      "224440    0\n",
      "148283    0\n",
      "56155     0\n",
      "7812      0\n",
      "         ..\n",
      "3461      0\n",
      "4792      0\n",
      "101428    0\n",
      "126394    0\n",
      "114578    0\n",
      "Name: TARGET, Length: 276759, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_numerical_features=len(numerical_cols)\n",
    "print(y_train)\n",
    "if isinstance(y_train,np.ndarray):\n",
    "    y_train=pd.Series(y_train.flatten())\n",
    "y_train = y_train.values.astype('float32').reshape(-1, 1)  # ç”¨äº BCE loss..è¿™æ ·y_trainå°±æ˜¯[n_samples,1]\n",
    "X_train_num_tensor = torch.tensor(x_train_numerical, dtype=torch.float32)\n",
    "X_train_cate_tensor = torch.tensor(x_train_cate_encoded[valid_categorical_cols].values, dtype=torch.long)     # åµŒå…¥å±‚çš„è¾“å…¥å¯¹äºåˆ†ç±»ç‰¹å¾å¿…é¡»æ˜¯torch.longçš„æ ¼å¼\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "train_dataset=TensorDataset(X_train_num_tensor,X_train_cate_tensor,y_train_tensor)         \n",
    "train_loader=DataLoader(train_dataset,batch_size=128,shuffle=True)\n",
    "\n",
    "if isinstance(y_test,np.ndarray):\n",
    "    y_test=pd.Series(y_test.flatten())\n",
    "y_test = y_test.values.astype('float32').reshape(-1, 1)  # ç”¨äº BCE loss\n",
    "X_test_num_tensor = torch.tensor(x_test_numerical, dtype=torch.float32)\n",
    "X_test_cate_tensor= torch.tensor(x_test_cate_encoded[valid_categorical_cols].values,dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "test_dataset=TensorDataset(X_test_num_tensor,X_test_cate_tensor,y_test_tensor)         \n",
    "test_loader=DataLoader(test_dataset,batch_size=128,shuffle=False)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b0d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹ DAE é¢„è®­ç»ƒ...\n",
      "Pretrain Epoch 1/10 | MSE Loss: 0.537340\n",
      "Pretrain Epoch 2/10 | MSE Loss: 0.181879\n",
      "Pretrain Epoch 3/10 | MSE Loss: 0.168756\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–æ¨¡å‹\n",
    "num_numerical_features=len(numerical_cols)\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=Tabular_Transformer(num_numerical=num_numerical_features,embed_dim=32,cate_dims_ls=cate_dims_ls,num_blocks=3,dropout=0.15)\n",
    "#model = DAE_MLP_Network(num_numerical=num_numerical_features, emb_dims=emb_dims).to(device)\n",
    "# ä¸“é—¨çš„ä¼˜åŒ–å™¨\n",
    "pretrain_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "# æ•°å€¼ç‰¹å¾ç”¨ HuberLoss æˆ– SmoothL1Loss (æŠ—å¼‚å¸¸å€¼)ã€‚ã€‚å®è´¨å‡½æ•°æ˜¯MSEçš„è¿›é˜¶ç‰ˆæœ¬\n",
    "criterion_num = nn.HuberLoss(delta=1.0) \n",
    "# ç±»åˆ«ç‰¹å¾ç”¨ CrossEntropyLoss   ï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰\n",
    "criterion_cate = nn.CrossEntropyLoss()\n",
    "print(\"ğŸš€ å¼€å§‹ DAE é¢„è®­ç»ƒ...\")\n",
    "EPOCHS_PRETRAIN = 10\n",
    "loss_history=[]\n",
    "for epoch in range(EPOCHS_PRETRAIN):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # éå† DataLoader (è¿™é‡Œçš„ y åœ¨é¢„è®­ç»ƒä¸­æ²¡ç”¨)\n",
    "    for x_num, x_cate, _ in train_loader:\n",
    "        x_num, x_cate = x_num.to(device), x_cate.to(device)    \n",
    "        # å‰å‘ä¼ æ’­ (é¢„è®­ç»ƒæ¨¡å¼)\n",
    "        # è¾“å‡ºï¼šç±»åˆ«ç‰¹å¾çš„é¢„æµ‹ï¼Œæ•°å€¼ç‰¹å¾çš„é¢„æµ‹\n",
    "        pred_num = model.forward_pretrain(x_cate, x_num)\n",
    "        # è®¡ç®— Lossï¼šæ¨¡å‹å­¦åˆ°çš„ç‰¹å¾å’ŒçœŸå®ç‰¹å¾ä¹‹é—´çš„å·®å€¼\n",
    "        loss_num = criterion_num(pred_num,x_num)\n",
    "        loss_cate=0\n",
    "        # å¿½ç•¥å¯¹ç±»åˆ«ç‰¹å¾çš„è®¡ç®—\n",
    "        # for i,pred_cate in enumerate(pred_cates):\n",
    "        #     #print(pred_cate.shape)  \n",
    "        #     #print(x_cate.shape)     # ï¼ˆ1024ï¼Œ102ï¼‰\n",
    "        #     loss_cate +=criterion_cate(pred_cate,x_cate[:,i])\n",
    "        loss=loss_num     #+(loss_cate/len(pred_cates))\n",
    "        \n",
    "        pretrain_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        pretrain_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Pretrain Epoch {epoch+1}/{EPOCHS_PRETRAIN} | MSE Loss: {avg_loss:.6f}\")\n",
    "torch.save(model.state_dict(),\"pretrain_dae.pth\")\n",
    "print(\"âœ… DAE é¢„è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ç»å­¦ä¼šäº†æ•°æ®çš„å†…éƒ¨ç»“æ„ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb50fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_each_epoch(model,train_loader,device,loss,optimizer,auc_metric):\n",
    "    metrics = {\n",
    "        'loss': 0.0,\n",
    "        'samples': 0,\n",
    "    }\n",
    "    model.train()         # åˆ‡æ¢ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "    # åœ¨æ¯ä¸ª epoch å¼€å§‹æ—¶ï¼Œé‡ç½® metric çš„å†…éƒ¨çŠ¶æ€\n",
    "    auc_metric.reset()\n",
    "    for X_num,X_cate,y in train_loader:\n",
    "        X_num,X_cate, y = X_num.to(device), X_cate.to(device),y.to(device)\n",
    "        y_hat,_=model.forward_classify(X_cate,X_num)\n",
    "        l=loss(y_hat,y)   # å› ä¸ºnn.BCEWithLogitsLoss()é»˜è®¤è¿”å›çš„æ˜¯å¹³å‡æŸå¤±ï¼Œæ‰€ä»¥è¦ç®—æ€»æŸå¤±è¿˜è¦ç”¨  l*æ ·æœ¬æ•°\n",
    "\n",
    "        # æ›´æ–°è¿‡ç¨‹\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step() #æ›´æ–°æƒé‡\n",
    "        \n",
    "        batch_size=y.numel()                        # ä¹Ÿå¯ä»¥ä½¿ç”¨y.size(0)\n",
    "        metrics[\"loss\"] += l.item()*batch_size      # l.item()è¿”å›çš„æ˜¯ä¸€ä¸ªpythonçš„æ•°å€¼\n",
    "        metrics[\"samples\"] +=y.numel()\n",
    "\n",
    "         # (æ–°å¢) æ›´æ–°AUC metric\n",
    "        # .update() ä¼šç´¯è®¡æ¯ä¸ªæ‰¹æ¬¡çš„ç»“æœ\n",
    "        # æ³¨æ„ï¼štorchmetrics éœ€è¦æ•´æ•°æ ‡ç­¾ï¼Œæ‰€ä»¥ç”¨ .int()\n",
    "        auc_metric.update(y_hat,y.int())\n",
    "    metrics[\"auc\"]  = auc_metric.compute().item()\n",
    "    return metrics\n",
    "def eval_each_epoch(model,test_loader,device,loss,auc_metric):\n",
    "    metrics = {\n",
    "        'loss': 0.0,\n",
    "        'samples': 0\n",
    "    }\n",
    "    model.eval()       # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼\n",
    "    auc_metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for X_num,X_cate,y in test_loader:\n",
    "            X_num,X_cate, y = X_num.to(device), X_cate.to(device),y.to(device)\n",
    "            y_hat,latent_val=model.forward_classify(X_cate,X_num)\n",
    "            l=loss(y_hat,y)   # å› ä¸ºnn.BCEWithLogitsLoss()é»˜è®¤è¿”å›çš„æ˜¯å¹³å‡æŸå¤±ï¼Œæ‰€ä»¥è¦ç®—æ€»æŸå¤±è¿˜è¦ç”¨  l*æ ·æœ¬æ•°\n",
    "            batch_size=y.size(0)\n",
    "            metrics[\"loss\"] += l.item()*batch_size      # l.item()è¿”å›çš„æ˜¯ä¸€ä¸ªpythonçš„æ•°å€¼\n",
    "            metrics[\"samples\"] +=y.numel()\n",
    "            auc_metric.update(y_hat,y.int())\n",
    "    metrics[\"auc\"]  = auc_metric.compute().item()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_epoches(model,train_loader,test_loader,device,loss,optimizer,epoches=20):\n",
    "    train_auc_metric = BinaryAUROC().to(device)\n",
    "    val_auc_metric = BinaryAUROC().to(device)\n",
    "    patience = 5 \n",
    "    patience_counter=0\n",
    "    best_val_auc=float('-inf')\n",
    "    eval_auc_epoches=[]\n",
    "    for epoch in range(epoches):\n",
    "        train_metrics = train_each_epoch(model,train_loader,device,loss,optimizer,auc_metric=train_auc_metric)\n",
    "        eval_metrics  = eval_each_epoch(model,test_loader,device,loss,auc_metric=val_auc_metric)\n",
    "        avg_loss=train_metrics['loss'] / train_metrics['samples']\n",
    "        train_auc=train_metrics[\"auc\"]\n",
    "\n",
    "        eval_avg_loss = eval_metrics['loss'] / eval_metrics['samples']\n",
    "        eval_auc=eval_metrics[\"auc\"]\n",
    "        eval_auc_epoches.append(eval_auc)\n",
    "\n",
    "        if eval_auc>best_val_auc:\n",
    "            best_val_auc=eval_auc\n",
    "            print(f\"ä¿å­˜çš„æ˜¯ç¬¬{epoch}è½®çš„æ¨¡å‹\")\n",
    "            torch.save(model.state_dict(),'best_model.pth')\n",
    "            print(f\"Epoch {epoch}: New best model saved with Val AUC: {eval_auc:.4f},,,,,{best_val_auc:.4f}\")\n",
    "            patience_counter = 0 # é‡ç½®è€å¿ƒè®¡æ•°å™¨\n",
    "        else:\n",
    "            patience_counter +=1\n",
    "        if patience_counter >=patience:\n",
    "            print(f\"æ¨¡å‹åœ¨ç¬¬{epoch-5}è½®ä¸­æ¥ä¸‹æ¥æŒç»­{patience}è½®æ— æ³•åšåˆ°AUCæ›´å¥½ï¼Œåœæ­¢æ¨¡å‹çš„è®­ç»ƒ\")\n",
    "            #break\n",
    "    return model,eval_auc_epoches\n",
    "# é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ (è¿™æ˜¯å¾®è°ƒçš„å…³é”®ï¼Œå­¦ä¹ ç‡é€šå¸¸æ¯”é¢„è®­ç»ƒå°ä¸€ç‚¹ï¼Œæˆ–è€…ç»“åˆ OneCycleLR)\n",
    "#finetune_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# åˆ†ç±»æŸå¤±å‡½æ•°\n",
    "#criterion = nn.BCEWithLogitsLoss() \n",
    "#train_epoches(model=model,train_loader=train_loader,test_loader=test_loader,device=device,loss=criterion,optimizer=finetune_optimizer,epoches=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹é¢è¿›è¡Œ5æŠ˜åˆ†å±‚ç¥ç»ç½‘ç»œè®­ç»ƒ\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf=StratifiedKFold(n_splits=5,random_state=108,shuffle=True)\n",
    "nn_folds=list(kf.split(x_train_numerical,y_train))\n",
    "\n",
    "def train_NN_oof(x_train_numerical, x_train_cate_encoded, y_train,x_test_numerical,x_test_cate_encoded, y_test,folds):\n",
    "    if isinstance(y_train,pd.Series):\n",
    "        y_train=y_train.values\n",
    "    oof = np.zeros(len(y_train))\n",
    "    oof_latents = np.zeros((len(y_train), 512), dtype=np.float32)\n",
    "    # ç”¨äºæµ‹è¯•é›†çš„éƒ¨åˆ†\n",
    "    test_latents_all=np.zeros((len(x_test_numerical), 512), dtype=np.float32)\n",
    "    preds_test = np.zeros(len(x_test_numerical))\n",
    "     # åœ¨æœ€ä½³æ¨¡å‹é¢„æµ‹ä¸‹è¿›è¡Œ\n",
    "    y_test = y_test.astype('float32').reshape(-1, 1)  # ç”¨äº BCE loss\n",
    "    X_test_num_tensor = torch.tensor(x_test_numerical, dtype=torch.float32)\n",
    "    X_test_cate_tensor= torch.tensor(x_test_cate_encoded[valid_categorical_cols].values,dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "    test_dataset=TensorDataset(X_test_num_tensor,X_test_cate_tensor,y_test_tensor)         \n",
    "    test_loader=DataLoader(test_dataset,batch_size=1024,shuffle=False)\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(folds):\n",
    "        print(f\"=== NN Fold {fold+1} ===\")\n",
    "        print(valid_idx.max()) \n",
    "        y_train_idx = y_train[train_idx].astype('float32').reshape(-1, 1)                # ç”¨äº BCE loss..è¿™æ ·y_trainå°±æ˜¯[n_samples,1]\n",
    "        X_train_num_tensor_idx = torch.tensor(x_train_numerical[train_idx], dtype=torch.float32)\n",
    "        X_train_cate_tensor_idx = torch.tensor(x_train_cate_encoded[valid_categorical_cols].values[train_idx], dtype=torch.long)     # åµŒå…¥å±‚çš„è¾“å…¥å¯¹äºåˆ†ç±»ç‰¹å¾å¿…é¡»æ˜¯torch.longçš„æ ¼å¼\n",
    "        y_train_tensor_idx = torch.tensor(y_train_idx, dtype=torch.float32)\n",
    "        train_dataset_idx=TensorDataset(X_train_num_tensor_idx,X_train_cate_tensor_idx,y_train_tensor_idx)         \n",
    "        train_loader_idx=DataLoader(train_dataset_idx,batch_size=1024,shuffle=True)\n",
    "\n",
    "\n",
    "        y_val_idx = y_train[valid_idx].astype('float32').reshape(-1, 1)  # ç”¨äº BCE loss\n",
    "        X_val_num_tensor_idx = torch.tensor(x_train_numerical[valid_idx], dtype=torch.float32)\n",
    "        X_val_cate_tensor_idx= torch.tensor(x_train_cate_encoded[valid_categorical_cols].values[valid_idx],dtype=torch.long)\n",
    "        y_val_tensor_idx = torch.tensor(y_val_idx, dtype=torch.float32)\n",
    "        val_dataset_idx=TensorDataset(X_val_num_tensor_idx,X_val_cate_tensor_idx,y_val_tensor_idx)         \n",
    "        val_loader_idx=DataLoader(val_dataset_idx,batch_size=1024,shuffle=False)\n",
    "        \n",
    "        # åˆ†ç±»æŸå¤±å‡½æ•°,ä½¿ç”¨BCEWithLogitsLosså‡½æ•°  ç”¨äºäºŒåˆ†ç±»ï¼ˆbinary classificationï¼‰ ä»»åŠ¡çš„æŸå¤±è®¡ç®—ã€‚æœ¬è´¨æ˜¯ signmodå‡½æ•°å’Œäº¤å‰ç†µæŸå¤±å‡½æ•°çš„èåˆ\n",
    "        criterion = nn.BCEWithLogitsLoss() \n",
    "        model.load_state_dict(torch.load('pretrain_dae.pth'), strict=False)\n",
    "\n",
    "        # ä½¿ç”¨é¢„è®­ç»ƒçš„è¿‡ç¨‹è¿›è¡Œåˆå§‹åŒ–æƒé‡,è¿™æ—¶å€™è¦æŠŠé™¤äº†classifier_headçš„ç»™é”ä½ï¼Œä¹Ÿå°±æ˜¯ä¸èƒ½è¿›è¡Œåå‘ä¼ æ’­äº†\n",
    "        for name,param in model.named_parameters():\n",
    "            if \"classifier_head\" not in name:\n",
    "                param.requires_grad=False\n",
    "        first_optimizer=torch.optim.AdamW(filter(lambda x:x.requires_grad,model.parameters()),lr=1e-4)\n",
    "        model.train()\n",
    "        for X_num,X_cate,y in train_loader_idx:\n",
    "            X_num,X_cate, y = X_num.to(device), X_cate.to(device),y.to(device)\n",
    "            y_hat,_=model.forward_classify(X_cate,X_num)\n",
    "            l=criterion(y_hat,y)   # å› ä¸ºnn.BCEWithLogitsLoss()é»˜è®¤è¿”å›çš„æ˜¯å¹³å‡æŸå¤±ï¼Œæ‰€ä»¥è¦ç®—æ€»æŸå¤±è¿˜è¦ç”¨  l*æ ·æœ¬æ•°\n",
    "            # æ›´æ–°è¿‡ç¨‹\n",
    "            first_optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            first_optimizer.step() #æ›´æ–°æƒé‡\n",
    "\n",
    "        #  è¿™æ—¶å€™å†æ­£å¼å¼€å§‹è®­ç»ƒï¼Œè¯„ä¼°\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        finetune_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "        best_model,eval_auc_epoches=train_epoches(model=model,train_loader=train_loader_idx,test_loader=val_loader_idx,device=device,loss=criterion,optimizer=finetune_optimizer,epoches=7)\n",
    "        print(\"åœ¨éªŒè¯é›†ä¸Šçš„AUCå˜åŒ–è¿‡ç¨‹\")\n",
    "                # è®¾ç½®ç”»å¸ƒå¤§å°\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, 8),eval_auc_epoches , marker='o', linestyle='-', color='b', label='EVal AUC 7-Epoches')\n",
    "        # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾\n",
    "        plt.title('åœ¨éªŒè¯é›†ä¸Šçš„AUCå˜åŒ–è¿‡ç¨‹', fontsize=16)\n",
    "        plt.xlabel('Epochs', fontsize=12)\n",
    "        plt.ylabel('AUC', fontsize=12)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        best_model.eval()\n",
    "        test_preds_fold=[]\n",
    "        test_latents_fold=[]\n",
    "        with torch.no_grad():\n",
    "            # éªŒè¯é›†\n",
    "            best_model.load_state_dict(torch.load(\"best_model.pth\"),strict=False)\n",
    "            best_preds,latent_val=best_model.forward_classify(X_val_cate_tensor_idx,X_val_num_tensor_idx)\n",
    "            oof[valid_idx]=torch.sigmoid(best_preds).cpu().numpy().flatten()\n",
    "            oof_latents[valid_idx] = latent_val.cpu().numpy()\n",
    "            oof_latents_df=pd.DataFrame(oof_latents,columns=[f\"latent_{i}\" for i in range(oof_latents.shape[1])])\n",
    "            # æµ‹è¯•é›†ï¼ˆåˆ†batchå¤„ç†ï¼‰\n",
    "            for X_num,X_cate,y in test_loader:\n",
    "                X_num,X_cate, y = X_num.to(device), X_cate.to(device),y.to(device)\n",
    "                y_hat,latent_test=best_model.forward_classify(X_cate,X_num)\n",
    "                test_preds_fold.append(torch.sigmoid(y_hat).cpu().numpy().flatten())  # flattenï¼ˆï¼‰æ˜¯ç”¨äºæŠŠäºŒç»´æ•°ç»„å±•å¹³çš„æ“ä½œ  \n",
    "                test_latents_fold.append(latent_test.cpu().numpy())\n",
    "        test_latents_fold=np.vstack(test_latents_fold)\n",
    "        test_preds_fold = np.concatenate(test_preds_fold)\n",
    "        preds_test += test_preds_fold/5   # éœ€è¦å¯¹numpyæ‰å¯ä»¥è¿™æ ·ç›´æ¥é™¤ä»¥5\n",
    "        test_latents_all +=test_latents_fold/5\n",
    "        test_latents_df=pd.DataFrame(test_latents_all,columns=[f\"latent_{i}\" for i in range(test_latents_all.shape[1])])\n",
    "    return oof,preds_test,oof_latents,test_latents_all\n",
    "\n",
    "nn_oof,nn_preds_test,latents_val,latents_test=train_NN_oof(x_train_numerical, x_train_cate_encoded, y_train,x_test_numerical,x_test_cate_encoded,y_test, nn_folds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
